# Response Quality Enhancer

This repository contains a condensed version of a prompt designed to enhance the quality of responses from Large Language Models (LLMs).

## Original Source

The original prompt is based on the "Response Quality Enhancer" found in the [Awesome_GPT_Super_Prompting](https://github.com/CyberAlbSecOP/Awesome_GPT_Super_Prompting/blob/main/My%20Super%20Prompts/Response%20Quality%20Enhacer.md) repository by CyberAlbSecOP.

## Why a Condensed Version?

The original Response Quality Enhancer prompt, while highly effective, has two main limitations:

1. Token Consumption: The original prompt is quite lengthy, consuming a significant number of tokens. This can be problematic in contexts where token usage directly affects costs or where there are strict limits on input length.

2. Character Limit in ChatGPT: ChatGPT's custom instructions are limited to 1500 characters. The original prompt exceeds this limit, making it impossible to use in its entirety within ChatGPT's custom instruction feature.

By creating a condensed version, we address both of these issues:
- Reduced token consumption, potentially lowering costs and allowing for more efficient use of the available context window.
- Compatibility with ChatGPT's custom instruction character limit, enabling users to fully implement the quality enhancement features.

## Condensed Version

The condensed version in this repository was created using an AI-assisted technique to minimize token usage while maintaining the prompt's effectiveness. This version is optimized for LLMs and may not be easily readable by humans.

You can find the condensed prompt in the [prompt.txt](prompt.txt) file in this repository.

## Technique Used

The condensation technique involved:
1. Using an LLM (Claude) to rewrite the original prompt with minimal tokens.
2. Iterative refinement through follow-up prompts.
3. Focusing on preserving the prompt's core functionality and intent.

[... rest of the content remains the same ...]
