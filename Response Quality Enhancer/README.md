# Response Quality Enhancer

This repository contains a condensed version of a prompt designed to enhance the quality of responses from Large Language Models (LLMs).

## Original Source

The original prompt is based on the "Response Quality Enhancer" found in the [Awesome_GPT_Super_Prompting](https://github.com/CyberAlbSecOP/Awesome_GPT_Super_Prompting/blob/main/My%20Super%20Prompts/Response%20Quality%20Enhacer.md) repository by CyberAlbSecOP.

## Condensed Version

The condensed version in this repository was created using an AI-assisted technique to minimize token usage while maintaining the prompt's effectiveness. This version is optimized for LLMs and may not be easily readable by humans.

## Technique Used

The condensation technique involved:
1. Using an LLM (Claude) to rewrite the original prompt with minimal tokens.
2. Iterative refinement through follow-up prompts.
3. Focusing on preserving the prompt's core functionality and intent.

The actual condensed prompt is not disclosed in this README to maintain its effectiveness.

## Usage

If you're unsure about what this condensed prompt does, you can ask any LLM the following:

```
Explain this prompt:

[Insert the condensed prompt text here]
```

The LLM should be able to interpret and explain the prompt's purpose and functionality.

## Purpose

This condensed prompt aims to improve the quality of LLM responses by setting a high standard for relevance, practicality, specificity, personalization, and comprehensiveness.

## Contribution

Feel free to contribute to this project by suggesting improvements or reporting issues. Please ensure that any contributions maintain the spirit of the original prompt while optimizing for token efficiency.

## License

See the [LICENSE](LICENSE) file for details.
